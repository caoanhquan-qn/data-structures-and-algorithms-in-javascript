Data Structures and Algorithms in Javascript
---------------------
Big O Notation
https://www.bigocheatsheet.com/

In computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows
time complexity

input size = N

1. O(1)
2. O(log n) 
4. O(n)
6. O(n log n)
8. O(n^2) (quadratic)

big O time complexity 

constant > log n > √n > n > n log n > n^2 > 2^n > n! 

option + v : √
print array of N = O(N)

space complexity 
---------------------

what happens to the space that an algorithm takes up as the size of the input increases
we can also use big O notation to analyze space complexity: how much additional memory do we need to allocate in order to run the code in our algorithm?

sometimes you’ll hear the term auxiliary space complexity to refer to space required by the algorithm, not including space taken up by the inputs

unless otherwise noted, when we talk about space complexity, technically we’ll be talking about auxiliary space complexity 

space complexity in JS (rules of thumb)
- most primitives (booleans, numbers, undefined, null) are constant space
- Strings require O(n) space (where n is the string length)
- Reference types are generally O(n), where n is the length (for arrays) or the number of keys (for objects)

logarithm
this isn’t a math course, so here’s a rule of thumb
the logarithm of a number roughly measures the number of times you can divide that number by 2 before you get a value that’s less than or equal to one

=> logarithm complexity

who cares?
- certain searching algorithms have logarithmic time complexity
- efficient sorting algorithms involve logarithms 
- recursion sometimes involves logarithmic space complexity 

Recap
- To analyze the performance of an algorithm, we use Big O Notation
- Big O Notation can give us a high level understanding of the time or space complexity of an algorithm
- Big O Notation doesn’t care about precision, only about general trends  (linear? quadratic? constant?)
- The time or space complexity (as measured by Big O) depends only on the algorithm, not the hardware used to run the algorithm
- Big O Notation is everywhere, so get lots of practice

---------------------

when to use objects
- when you don’t need order
- when you need fast access/ insertion and removal

Big O of Objects

insertion - O(1)
removal - O(1)
searching - O(n)
access - O(1)
update - O(1)

when you don’t need any ordering, objects are an excellent choice!

Big O of Object methods

Object.keys - O(n)
Object.values - O(n)
Object.entries - O(n)
Object.hasOwnProperty - O(1)

---------------------

const jonas = {
  firstName: "Nick",
  age: 32,
  isSingle: true,
  score: [49, 64, 75, 85],
};

console.log(Object.keys(jonas));

// ["firstName", "age", "isSingle", "score"]

console.log(Object.values(jonas));

// ["Nick", 32, true, Array(4)]

console.log(Object.entries(jonas));

/*
0: (2) ["firstName", "Nick"]
1: (2) ["age", 32]
2: (2) ["isSingle", true]
3: (2) ["score", Array(4)]
*/

console.log(jonas.hasOwnProperty("score")); // true

---------------------

when to use arrays
- when you need order 
- when you need fast access/ insertion and removal

Big O of arrays

insertion - it depends
depends on where we’re inserting/removing 

if we add to the end of the array => O(1)
the problem comes when we try to insert at the beginning of an array. we have to re-index every single element in the array. 
If we’re talking about thousands and thousands of elements, re-indexing every single one is not a trivial task => O(n)
inserting in the beginning is problematic. the same goes for removing from the beginning ⬇︎

removal - it depends
remove from the beginning => O(n)
remove the last element => O(1)


=> adding and removing from the beginning of an array is best to avoid if you can. So push and pop are always faster than unshift and shift
push - unshift
pop - shift

unless it’s an empty array in which case adding to the beginning or end is the same thing

searching - O(n)

access - O(1)
it doesn’t matter how long the array is => jump immediately to the data 

Big O of array operations
- push - O(1)
- pop - O(1)
- shift - O(n)
- unshift - O(n)
- concat - O(n)
- slice - O(n)
- splice - O(n)
- sort - O(n * log n)
- forEach/ map/ filter/ reduce/ ... - O(n)

---------------------
Problem solving pattern
- Frequency counter
- Multiple pointers
- Sliding window
- Divide and conquer
- Dynamic programming
- Greedy algorithms
- Backtracking
- Many more! 

Frequency counter
This pattern uses objects or sets to collect values/ frequencies of values. This can often avoid the need for nested loops or O(n^2) operations with arrays/ strings

Multiple pointers (it’s not really an official name)

Creating pointers or values that correspond to an index or position and move towards the beginning, end or middle based on a certain condition. 
Very efficient for solving problems with minimal space complexity as well

Sliding window
This pattern involves creating a window which can either be an array or number from one position to another. 
Depending on a certain condition, the window either increases or closes (and a new window is created). Very useful for keeping track of a subset of data in an array/ string

Divide and Conquer pattern
This pattern involves dividing a data set into smaller chunks and then repeating a process with a subset of data. This pattern can tremendously decrease time complexity

---------------------
Recursion
What is recursion?
A process (a function in our case) that calls itself

How recursive functions work?
Invoke the same function with a different input until you reach your base case
base case: the condition when the recursion ends

Two essential parts of a recursive function
- base case
- different input

Common recursion pitfalls
- No base case
- Forgetting to return or returning the wrong thing
- stack overflow

stack overflow : maximum call stack size exceeded, too many frames on that stack, too many functions trying to be called. This means that your recursion is not stopped

Pure recursion tips
- For arrays, use methods like slice, the spread operator and concat that make copies of arrays so you do not mutate them
- Remember that strings are immutable so you will need to use methods like slice, substr or substring to make copies of strings
- To make copies of objects use Object.assign or the spread operator

---------------------

Searching Algorithms
- Linear search : it’s the best we can do with an unsorted piece of data. For example, search a table of N rows = O(n)
- Binary Search : only works on sorted arrays. The idea is divide and conquer pattern. Big O : O(log n)
- Naive string searching algorithm
- KMP string searching algorithm

There are many different search methods on arrays in Javascript
- indexOf
- includes
- find
- findIndex

How do these functions work?
All of these methods are doing the same thing behind the scenes.
They’re checking every element one at a time to check if whatever we pass in. 
They are going through and checking each item to see. It’s called a linear search.

Binary Search
- Binary search is a much faster form of search
- Rather than eliminating one element at a time, you can eliminate half of the remaining elements at a time
- Binary search only works on sorted arrays

Binary search pseudocode
- This function accepts a sorted array and a value
- Create a left pointer at the start of the array and a right pointer at the end of the array
- While the left pointer comes before the right pointer:
    * Create a pointer in the middle
    * If you find the value you want, return the index
    * If the value is too small, move the right pointer down
    * If the value is too large, move the left pointer up 
- If you never find the value, return -1

---------------------

Sorting Algorithms
https://visualgo.net/en/sorting
https://www.toptal.com/developers/sorting-algorithms

[
1. Bubble sort
2. Selection sort
3. Insertion sort
4. Shell sort ] => elementary sorting algorithms
[
5. Merge sort
6. Quick sort
7. Radix sort
8. Heap sort ] => intermediate sorting algorithms

Bubble sort : a sorting algorithm where the largest values bubble up to the top
Bubble sort pseudocode
- Start looping from the end of the array towards the beginning with a variable called i 
- Start an inner loop with a variable called j from the beginning until i - 1
-  If arr[j] is greater than arr[j+1], swap those two values
- Return the sorted array

What is the time complexity of bubble sort?
best case: O(n) where the data is nearly sorted 
worst case: O(n^2). In general, n^2 because we have a nested loop 

Selection sort : similar to bubble sort, but instead of first placing large values into sorted position at the end of the array, it places small values into sorted position at the beginning

Selection sort pseudocode
- Store the first element as the smallest value you’ve seen so far
- Compare this item to the next item in the array until you find a smaller number
- If a smaller number is found, designate that smaller number to be the new “minimum” and continue until the end of the array
- If the “minimum” is not the value (index) you initially began with, swap the two values
- Repeat this with the next element until the array is sorted

What is the time complexity of selection sort?
O(n^2)
selection sort potentially is better than something like bubble sort because we minimize the number of swaps that we’re making
bubble sort : we’re basically swapping over and over and over to get the largest item to the end, we keep swapping until it goes to the end and the next time through we swap if we need to keep swapping 

selection sort : we iterate we compare a lot but we only make one swap at the end of each loop   

Insertion sort : builds up the sort by gradually creating a larger left half which is always sorted 
Insertion sort pseudocode
- Start by picking the second element in the array
- Now compare the second element with the one before it and swap if necessary
- Continue to the next element and if it is in the incorrect order, iterate through the sorted portion (i.e. the left side) to place the element in the correct place
- Repeat until the array is sorted

What is the time complexity of insertion sort?
O(n^2)
It works very well if you need your data to be continuously sorted. You have data coming in and you need to re-sort things over and over and maintain a running sort, keep things up-to-date.

Insertion sort is an online algorithm
Selection sort is an offline algorithm

Divide and Conquer pattern 
Merge sort and Quick sort are examples of divide and conquer pattern 

Merge sort
- In order to implement merge sort, it’s useful to first implement a function responsible for merging two sorted arrays
- Given two arrays which are sorted, this helper function should create a new array which is also sorted and consists of all of the elements in the two input arrays
- This function should run in O(n + m) time and O(n + m) space and should not modify the parameters passed to it

merge helper pseudocode
- Create an empty array, take a look at the smallest values in each input array
- While there are still values we haven’t looked at
    - If the value in the first array is smaller than the value in the second array, push the value in the first array into our results and move on to the next in the first array
    - If the value in the first array is larger than the value in the second array, push the value in the second array into our results and move on to the next value in the second array
    - Once we exhaust one array, push in all remaining values from the other array 

mergeSort pseudocode
- Break up the array into halves until you have arrays that are empty or have one element (use slice)
- Once you have smaller sorted arrays, merge those arrays with other sorted arrays until you are back at the full length of the array
- Once the array has been merged back together, return the merged and sorted array

What is the time complexity of merge sort?
O(n log n)
space complexity : O(n) 

Quick sort
- Like merge sort, exploits the fact that arrays of 0 or 1 element are always sorted 
- Works by selecting one element (called the “pivot”) and finding the index where the pivot should end up in the sorted array
- Once the pivot is positioned appropriately, quick sort can be applied on either side of the pivot 

pivot helper 
- In order to implement quick sort, it’s useful to first implement a function responsible arranging elements in an array on either side of a pivot
- Given an array, this helper function should designate an element as the pivot
- It should then rearrange elements in the array so that all values less than the pivot are moved to the left of the pivot and all values greater than the pivot are moved to the right of the pivot
- The order of elements on either side of the pivot doesn’t matter
- The helper should do this in place, that is, it should not create a new array
- When complete, the helper should return the index of the pivot

picking a pivot
- The runtime of quick sort depends in part on how one selects the pivot
- Ideally, the pivot should be chosen so that it’s roughly the median value in the data set you’re sorting
- For simplicity, we’ll always choose the pivot to be the first element 

pivot helper pseudocode
- It will help to accept three arguments: an array, a start index, and an end index (these can default to 0 and the array length -1, respectively)
- Grab the pivot from the start of the array
- Store the current pivot index in a variable (this will keep track of where the pivot should end up)
- Loop through the array from the start until the end
    - If the pivot is greater than the current element, increment the pivot index variable and then swap the current element with the element at the pivot index
- Swap the starting element (i.e. the pivot) with the pivot index
- Return the pivot index

Quick sort pseudocode
- Call the pivot helper on the array
- When the helper returns to you the updated pivot index, recursively call the pivot helper on the subarray to the left of that index, and the subarray to the right of that index
- Your base case occurs when you consider a subarray with less than 2 elements

What is the time complexity of quick sort?
O(n log n)
worst case : O(n^2) where our data is already sorted 

Radix sort 

Radix sort helpers
- In order to implement radix sort, it’s helpful to build a few helper functions first:
    - getDigit(num, place) - returns the digit in num at the given place value
    - digitCount(num) - returns the number of digits in num
    - mostDigits(nums) - given an array of numbers, returns the number of digits in the largest number in the list 

Radix sort pseudocode
- Define a function that accepts list of numbers
- Figure out how many digits the largest number has 
- Loop from k = 0 up to this largest number of digits
- For each iteration of the loop:
    - Create buckets for each digit (0 to 9)
    - Place each number in the corresponding bucket based on its k-th digit
- Replace our existing array with values in our buckets, starting with 0 and going up to 9
- Return list at the end

What is the time complexity of radix sort?
O(nk)
space complexity : O(n + k)
n = length of array
k = number of digits 

best sorting algorithm = O(n log(n)) (such as merge sort, quick sort)

---------------------

Singly Linked List
---------------------

It’s a data structure that stores whatever data you want such as strings, numbers. It’s ordered. 
It’s a list of data just like an array. But there’s a really big distinction.
A data structure that contains a head, tail, and length property. 
Linked lists consist of nodes and each node has a value and a pointer to another node or null.
A node stores a piece of data like a string or a number but it also references the next node. 
Singly linked list comes from the fact that each node is only connected one directionally to the next node.
One of the things that linked lists are good at is insertion and deletion. 
One of the main reasons you would want to use linked lists is that you really care about insertion and deletion, 
especially if you’re working with really long piece of data and you don’t need random access.

Comparison with arrays

Lists	
- Do not have indexes	
- Connected via nodes with a next pointer	
- Random access is not allowed	
Arrays
- Indexed in order 
- Insertion and deletion can be expensive
- Can quickly be accessed at a specific index

Singly linked list methods
- push(val) : adding a new node to the end of the linked list
- pop() : removing a node from the end of the linked list
- shift() : removing a node from the beginning of the linked list 
- unshift(val) : adding a new node to the beginning of the linked list
- get(index) : retrieving a node by its position in the linked list
- set(index, val) : changing the value of a node based on its position in the linked list
- insert(index, val) : adding a new node to the linked list at a specific position
- remove(index) : removing a node from the linked list at a specific position
- reverse() : reversing the linked list in place

pushing pseudocode
- This function should accept a value
- Create a new node using the value passed to the function
- If there is no head property on the list, set the head and tail to be the newly created node
- Otherwise set the next property on the tail to be the new node and set the tail property on the list to be the newly created node
- Increment the length by 1
- Return the linked list

popping pseudocode
- If there are no nodes in the list, return undefined
- Check if this.length === 1 (edge case)
- Loop through the list until you reach the tail
- Set the next property of the 2nd to last node to be null
- Set the tail to be the 2nd to last node
- Decrement the length of the list by 1
- Return the value of the node removed

shifting pseudocode
- If there are no nodes, return undefined
- Store the current head property in a variable
- Set the head property to be the current head’s next property
- Decrement the length by 1
- Return the value of the node removed

unshifting pseudocode
- This function should accept a value
- Create a new node using the value passed to the function
- If there is no head property on the list, set the head and tail to be the newly created node
- Otherwise set the newly created node’s next property to be the current head property on the list
- Set the head property on the list to be that newly created node
- Increment the length of the list by 1
- Return the linked list

get pseudocode
- This function should accept an index
- If the index is less than 0 or greater than or equal to the length of the list, return null
- Loop through the list until you reach the index and return the node at that specific index

set pseudocode
- This function should accept an index and a value
- Use your get function to find the specific node  
- If the node is not found, return false
- If the node is found, set the value of that node to be the value passed to the function and return true

insert pseudocode
- If the index is less than 0 or greater than the length, return false
- If the index is the same as the length, push a new node to the end of the list
- If the index is 0, unshift a new node to the start of the list
- Otherwise, using the get method, access the node at the index - 1
- Set the next property on the new node to be the previous next
- Set the next property on the node [index-1] to be the new node
- Increment the length
- Return true 

remove pseudocode
- If the index is less than 0 or greater than or equal to the length, return undefined
- If the index is the same as the length -1, pop
- If the index is 0, shift
- Otherwise, using the get method, access the node at the index -1 
- Set the next property on that node to be the next of the next node
- Decrement the length
- Return the value of the node removed

reverse pseudocode
- Swap the head and tail
- Create a variable called next
- Create a variable called prev
- Create a variable called node and initialize it to the head property
- Loop through the list
- Set next to be the next property on whatever node is
- Set the next property on the node to be whatever prev is
- Set prev to be the value of the node variable
- Set the node variable to be the value of the next variable 

Big O of Singly Linked List
Insertion - O(1)
Removal - It depends ... O(1) or O(n)
Searching - O(n)
Access - O(n)
---------------------

Big O of Array
Insertion -  It depends ... O(1) or O(n)
Removal -  It depends ... O(1) or O(n)
Searching - O(n)
Access - O(1)

Recap
- Singly linked lists are an excellent alternative to arrays when insertion and deletion at the beginning are frequently required
- Arrays contain a built-in index whereas linked lists don’t
- The idea of a list data structure that consists of nodes is the foundation for other data structures like stacks and queues

---------------------
Doubly Linked List
---------------------

Almost identical to Singly Linked Lists, except every node has another pointer to the previous node

Doubly linked list methods
push(val) : adding a node to the end of the doubly linked list
pushing pseudocode
- Create a new node with the value passed to the function
- If the head property is null, set the head and tail to be the newly created node
- If not, set the next property on the tail to be that node
- Set the previous property on the newly created node to be the tail
- Set the tail to be the newly created node
- Increment the length
- Return the doubly linked list

pop() : removing a node from the end of the doubly linked list
popping pseudocode
- If there is no head, return undefined
- Store the current tail in a variable to return later
- If the length is 1, set the head and tail to be null
- Update the tail to be the previous node
- Set the newTail’s next to null
- Decrement the length
- Return the value removed

shift() : removing a node from the beginning of the doubly linked list
shifting pseudocode
- If length is 0, return undefined
- Store the current head property in a variable (we’ll call it old head)
- If the length is 1, set the head to be null, set the tail to be null
- Update the head to be the next of the old head
- Set the head’s prev property to null
- Set the old head’s next to null
- Decrement the length
- Return old head

unshift(val) : adding a node to the beginning of the doubly linked list
unshifting pseudocode
- Create a new node with the value passed to the function
- If the length is 0, set the head to be the new node, set the tail to be the new node
- Otherwise, set the prev property on the head of the list to be the new node, set the next property on the new node to be the head property
- Update the head to be the new node
- Increment the length
- Return the list

get(index) : accessing a node in a doubly linked list by its position
get pseudocode
- If the index is less than 0 or greater or equal to the length, return null
- If the index is less than or equal to half the length of the list, loop through the list starting from the head and loop towards the middle, return the node once it is found
- If the index is greater than half the length of the list, loop through the list starting from the tail and loop towards the middle, return the node once it is found

set(index, value) : replacing the value of a node to the in a doubly linked list
set pseudocode
- Create a variable which is the result of the get method at the index passed to the function
- If the get method returns a valid node, set the value of that node to be the value passed to the function, return true
- Otherwise, return false

insert(index, value) : adding a node in a doubly linked list by a certain position
insert pseudocode
- If the index is less than 0 or greater than to the length, return false
- If the index is 0, unshift
- If the index is the same as the length, push
- Use the get method to access the index-1
- Set the next and prev properties on the correct nodes to link everything together
- Increment the length
- Return true

remove(index) : removing a node in a doubly linked list by a certain position
remove pseudocode
- If the index is less than 0 or greater than or equal to the length, return undefined
- If the index is 0, shift
- If the index is the same as the length -1, pop
- Use the get method to retrieve the item to be removed
- Update the next and prev properties to remove the found node from the list
- Set next and prev to null on the found node
- Decrement the length
- Return the removed node

Big O of doubly linked list
Insertion - O(1)
Removal - O(1)
Searching - O(n)
Access - O(n)
Technically searching is O(n/2) but that’s still O(n)

Recap
- Doubly linked lists are almost identical to singly linked lists except there is an additional pointer to previous node
- Better than singly linked lists for finding nodes and can be done in half the time
- However, they do take up more memory considering the extra pointer

---------------------
Stack and Queue
---------------------

Stack
A stack is just a collection of data 
A LIFO data structure. (LIFO = Last In First Out)
The last element added to the stack will be the first element removed from the stack

How is it used?
Think about a stack of plates, or a stack of markers, or a stack of anything. As you pile it up the last thing (or the topmost thing) is what gets removed first. 

There’s more than one way to implement a stack but you could use a linked list 

Where stacks are used
- managing function invocations
- undo/ redo
- routing (the history object) is treated like a stack

In fact, a stack is just a concept. It’s basically a set of rules, the first thing added in is the last thing removed and the last thing added in is the first thing removed. There’s more than one way of doing it. And the easiest way is to use an array

Writing our own stack
pushing : add a value to the top of the stack
pushing pseudocode
- The function should accept a value
- Create a new node with that value
- If there are no nodes in the stack, set the first and last property to be the newly created node
- If there is at least one node, create a variable that stores the current first property on the stack
- Reset the first property to be the newly created node
- Set the next property on the node to be the previously created variable
- Increment the size of the stack by 1
- Return the size property

pop pseudocode
- If there are no nodes in the stack, return null
- Create a temporary variable to store the first property on the stack
- If there is only 1 node, set the first and last property to be null
- If there is more than one node, set the first property to be next property on the current first
- Decrement the size by 1
- Return the value of the node removed

Big O of stack
Insertion - O(1)
Removal - O(1)
Searching - O(n)
Access - O(n)

Stack is really prioritizing insertion and removal
Recap
- Stacks are a LIFO data structure where the last value in is always the first one out
- Stacks are used to handle function invocations (the call stack) for operations like undo/ redo and for routing (remember pages you have visited and go back/ forward) and much more
- They are not a built-in data structure in Javascript, but are relatively simple to implement

---------------------
Queue

A FIFO data structure (FIFO = First In First Out)

Think of a line, the first person in line is the first person out.
The first piece of data in a queue is the first thing out

How do we use them in programming?
- Background tasks
- Uploading resources
- Printing/ Task processing

enqueue pseudocode
- This function accepts some value
- Create a new node using that value passed to the function
- If there are no nodes in the queue, set this node to be the first and last property of the queue
- Otherwise, set the next property on the current last to be that node, and then set the last property of the queue to be that node
- Increment the size of the queue by 1 and return that

dequeue pseudocode
- If there is no first property, just return null
- Store the first property in a variable
- See if the first is the same as the last (check if there is only 1 node). If so, set the first and last to be null
- If there is more than 1 node, set the first property to be the next property of first 
- Decrement the size by 1
- Return the value of the node dequeued

Big O of queues
Insertion - O(1)
Removal - O(1)
Searching - O(n)
Access - O(n)

Recap 
- Queues are a FIFO data structure, all elements are first in first out
- Queues are useful for processing tasks and are foundational for more complex data structures
- Insertion and removal can be done in O(1)

---------------------
Trees
---------------------
https://www.freecodecamp.org/news/all-you-need-to-know-about-tree-data-structures-bceacb85490c/

A data structure that consists of nodes in a parent/ child relationship
Linked List : linear data structure
Tree : non-linear data structure
Tree terminology
- root : the top node in a tree
- child : a node directly connected to another node when moving away from the root
- parent : the converse notion of a child
- siblings : a group of nodes with the same parent
- leaf : a node with no children
- edge : the connection between one node and another

Trees - lots of different applications
- HTML DOM
- Network routing
- Abstract syntax tree
- Artificial Intelligence
- Folders in operating systems
- Computer file systems

Trees
---------------------

Binary trees

Binary tree is a special type of tree. It has a special condition. Each node can have at most two children at the binary part. So it can have 0 children, can have 1 child, and can have 2 children. So the root node usually would have 2 and then each node could have 2 or could have 1 or it could have 0 but it can’t have 3. 

Binary trees have some special properties that make them easier to navigate 
---------------------
Binary search trees
Binary search trees are a special type of a binary tree. And they excel as you can imagine from the title at searching. We store sorted data in a particular way. 
Binary search tree makes it easier to retrieve 
They are sorted in a particular way, they are kept in an order. BST(Binary Search Tree) are used to store data that can be compared. That is sortable.

How BSTS work?
- Every parent node has at most 2 children 
- Every node to the left of a parent node is always less than the parent
- Every node to the right of a parent node is always greater than the parent 

BST: insert
inserting pseudocode
steps - iteratively or recursively
- Create a new node
- Starting at the root
- Check if there is a root, if not - the root now becomes that new node
- If there is a root, check if the value of the new node is greater than or less than the value of the root
- If it is greater, check to see if there is node to the right. If there is, move to that node and repeat these steps. If there is not, add that node as the right property (exit the loop)
- If it is less, check to see if there is a node to the left. If there is, move to that node and repeat these steps. If there is not, add that node as the left property (exit the loop)
- Return the entire tree at the very end

https://www.geeksforgeeks.org/implementation-binary-search-tree-javascript/
Recursively

BST: find
finding a node in a BST
steps - iteratively or recursively
- Start at the root
- Check if there is a root, if not - we’ve done searching 
- If there is a root, check if the value of the new node is the value we are looking for. If we found it, we’re done
- If not, check to see if the value is greater than or less than the value of the root 
- If it is greater, check to see if there is a node to the right. If there is, move to that node and repeat these steps. If there is not, we’ve done searching
- If it is less, check to see if there is a node to the left. If there is, move to that node and repeat these steps. If there is not, we’ve done searching

Big O of BST
Insertion - O(log n)
Searching - O(log n)

but NOT guaranteed

the worst case: O(n)
---------------------
b-tree : the core of most databases implementations 
b-tree : a self-balancing tree data structures that keeps data sorted and allows searches, sequential access, insertions and deletions in logarithmic time. It is commonly used in databases and filesystems

b-tree of order m = 3, meaning a node has at most 3 children 

---------------------
Tree traversal
https://en.wikipedia.org/wiki/Tree_traversal

Two ways:
- Breadth-first search (BFS)
- Depth-first search (DFS)
DFS
- Inorder
- Pre-order
- Post-order

BFS 
steps - iteratively
- Create a queue (this can be an array) and a variable to store the values of nodes visited 
- Place the root node in the queue
- Loop as long as there is anything in the queue
- Dequeue a node from the queue and push the value of the node into the variable that stores the nodes
- If there is a left property on the node dequeued - add it to the queue
- If there is a right property on the node dequeued - add it to the queue
- Return the variable that stores the values

DFS - PreOrder
Steps - Recursively
- Create a variable to store the values of nodes visited
- Store the root of the BST in a variable called current
- Write a helper function which accepts a node
- Push the value of the node to the variable that stores the values
- If the node has a left property, call the helper function with the left property on the node
- If the node has a right property, call the helper function with the right property on the node
- Invoke the helper function with the current variable
- Return the array of values

DFS - PostOrder
Steps - Recursively
- Create a variable to store the values of nodes visited
- Store the root of the BST in a variable called current
- Write a helper function which accepts a node
- If the node has a left property, call the helper function with the left property on the node
- If the node has a right property, call the helper function with the right property on the node
- Push the value of the node to the variable that stores the values
- Invoke the helper function with the current variable
- Return the array of values 

DFS - InOrder
Steps - Recursively
- Create a variable to store the values of nodes visited
- Store the root of the BST in a variable called current 
- Write a helper function which accepts a node
- If the node has a left property, call the helper function with the left property on the node
- Push the value of the node to the variable that stores the values
- If the node has a right property, call the helper function with the right property on the node
- Invoke the helper function with the current variable
- Return the array of values 

Recap
- Trees are non-linear data structures that contain a root and child nodes
- Binary trees can have values of any type, but at most two children for each parent
- Binary search trees are a more specific version of binary trees where every node to the left of a parent is less than its value and every node to the right is greater 
- We can search through trees using BFS and DFS

---------------------
Heaps
---------------------
https://en.wikipedia.org/wiki/Heap_(data_structure)
https://en.wikipedia.org/wiki/List_of_data_structures

They are trees

What is a binary heap?

Very similar to a binary search tree, but with some different rules.
In a MaxBinaryHeap, parent nodes are always larger than child nodes. In a MinBinaryHeap, parent nodes are always smaller than child nodes.

Max binary heap
- Each parent has at most two child nodes
- The value of each parent node is always greater than its child nodes
- In a max binary heap, the parent is greater than the children, but there are no guarantees between sibling nodes
- A binary heap is as compact as possible. All the children of each node are as full as they can be an and left children are filled out first 

Why do we need to know this?
- Binary heaps are used to implement priority queues, which are very commonly used data structures
- They are also used quite a bit, with graph traversal algorithms

---------------------

There’s an easy way of storing a binary heap. We can use an array to store a binary heap. For any index of an array (n). The left child is stored at 2n + 1. The right child is at 2n + 2. 
What if we have a child node and want to find its parent? 
For any child node at index n. Its parent is at index Math.floor((n-1)/2)

heap: insert
insert pseudocode
- Push the value into the values property on the heap
- Bubble the value up to its correct spot
- Bubble up
- Create a variable called index which is the length of the values property -1
- Create a variable called parentIndex which is the floor of (index -1)/2
- Keep looping as long as the values element at the parentIndex is less than the values element at the child index
- Swap the value of the values element at the parentIndex with the value of the element property at the child index
- Set the index to be the parentIndex and start over

heap: extract max (removing)
- Remove the root
- Replace with the most recently added
- Adjust (sink down)

pseudocode
- Swap the first value in the values property with the last one
- Pop from the values property, so you can return the value at the end
- Have the new root “sink down” to the correct spot
- Your parent index starts at 0 (the root)
- Find the index of the left child: 2*index  + 1 (make sure it’s not out of bounds)
- Find the index of the right child: 2*index  + 2 (make sure it’s not out of bounds)
-  If the left or right child is greater than the element... swap. If both left and right children are larger, swap with the largest child
- The child index you swapped to now becomes the new parent index
- Keep looping and swapping until neither child is larger than the element
- Return the old root

---------------------

Priority queue

A data structure where each element has a priority. Elements with higher priorities are served before elements with lower priorities

val doesn’t matter. Heap is constructed using priority

pseudocode
- Write a min binary heap - lower number means higher priority
- Each node has a val and a priority. Use the priority to build the heap
- Enqueue method accepts a value and priority, makes a new node, and puts it in the right spot based off of its priority
- Dequeue method removes root element, returns it and rearranges heap using priority

Big O of binary heaps 
- Insertion - O(log n)
- Removal - O(log n)
- Search - O(n)
the worst case: O(log n)

Remember that when we insert into a heap we always fill out the left-side first. Unlike a binary search tree where we might have an incomplete level. In binary heap, everything is always filled out before we move on to the next level 

Recap
- Binary heaps are very useful data structures for sorting, and implementing other data structures like priority queues
- Binary heaps are either MaxBinaryHeaps or MinBinaryHeaps with parents either larger or smaller than their children
- With just a little bit of math, we can represent heaps using arrays

---------------------
Hash table (hash map)
---------------------

Hash tables are used to store key-value pairs. They are like arrays, but the keys are not ordered
Unlike arrays, hash tables are fast for all of the following operations: finding values, adding new values and removing values. 

Why should we care?
- Nearly every programming language has some sort of hash table data structure
- Because of their speed, hash tables are very commonly used

Hash tables in the wild
- Python has Dictionaries
- Javascript has Object Literals and Maps
- Java, Go and Scala have Maps
- Ruby has Hashes

Hash function
- To implement a hash table, we’ll be using an array.
- In order to look up values by key, we need a way to convert keys into valid array indices
- A function that performs this task is called a hash function

hash = convert keys into valid array indices

https://en.wikipedia.org/wiki/Hash_function

What makes a good hash?
- Fast (i.e. constant time)
- Doesn’t cluster outputs at specific indices, but distributes uniformly
- Deterministic (same input yields same output)
deterministic = it means that every time we pass in one input we always get the same output, it’s determined that we’re going to get the same output. Same input to always give the same output

Hash functions almost always take advantage of prime numbers. There’s a couple of reasons but it comes down to reduce collisions. It means that we don’t want data to be stored in the same buckets. We want to make sure that we are spreading data out as much as possible so that it’s faster to retrieve.

Prime numbers?
- The prime number in the hash is helpful in spreading out the keys more uniformly
- It’s also helpful if the array that you’re putting values into has a prime length

Dealing with collisions
- Even with a large array and a great hash function, collisions are inevitable
- There are many strategies for dealing with collisions, but we’ll focus on two: separate chaining, linear probing

Separate chaining
- With separate chaining, at each index in our array we store values using a more sophisticated data structure (e.g. an array or a linked list)
- This allows us to store multiple key-value pairs at the same index 

Linear probing
- With linear probing, when we find a collision, we search through the array to find the next empty slot
- Unlike with separate chaining, this allows us to store a single key-value at each index

set/get
set
- Accepts a key and a value
- Hashes the key
- Stores the key-value pair in the hash table array via separate chaining

get
- Accepts a key
- Hashes the key
- Retrieves the key-value pair in the hash table
- If the key isn’t found, returns undefined

keys/values
keys
- Loops through the hash table array and returns an array of keys in the table

values
- Loops through the hash table array and returns an array of values in the table

Big O of hash tables (average case)
Insert - O(1)
Deletion - O(1)
Access - O(1)

Recap
- Hash tables are collections of key-value pairs
- Hash tables can find values quickly given a key
- Hash tables can add new key-values quickly
- Hash tables store data in a large array and work by hashing the keys
- A good hash should be fast, distribute keys uniformly, and be deterministic
- Separate chaining and linear probing are two strategies used to deal with two keys that hash to the same index

Graph
---------------------

A graph data structure consists of a finite (and possibly mutable) set of vertices or nodes or points, together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph

Graph = Nodes + Connections

Uses for graphs
- Social Networks
- Location/ Mapping
- Routing Algorithms
- Visual Hierarchy
- File System Optimizations
- Everywhere

Essential graph terms
- Vertex - a node
- Edge - connection between nodes
- Weighted/ Unweighted - values assigned to the edges between vertices 
- Directed/ Undirected - directions assigned to the edges between vertices 

Types of graph
- Undirected graph
- Directed graph
- Weighted graph. When we assign values to the edges, it becomes a weighted graph 
- Unweighted graph
  
Storing graphs
Adjacency List and Adjacency Matrix

Adjacency List
- Can take up less space (in sparse graphs)
- Faster to iterate over all edges
- Can be slower to lookup specific edge

---------------------

const hashmap = {
  A: ["B", "C"],
  B: ["A", "D"],
  C: ["A", "E"],
  D: ["B", "E", "F"],
  E: ["C", "D", "F"],
  F: ["D", "E"],
}

A : vertex
["B", "C"] : neighboring vertices of vertex A

---------------------

Adjacency Matrix
- Takes up more space (in sparse graphs)
- Slower to iterate over all edges
- Faster to look up specific edge

Differences and Big O
|V| - number of vertices
|E| - number of edges

Operation	Adjacency list	Adjacency matrix
Add vertex	O(1)	O(|V^2|)
Add edge	O(1)	O(1)
Remove vertex	O(|V| + |E|)	O(|V^2|)
Remove edge	O(|E|)	O(1)
Query	O(|V| + |E|)	O(1)
Storage	O(|V| + |E|)	O(|V^2|)

What will we use?
An adjacency list
Most data in the real world tends to lend itself to sparser and/ or larger graphs

using nested array or hash table to construct adjacency list


adding a vertex
- Write a method called addVertex, which accepts a name of a vertex
- It should add a key to the adjacency list with the name of the vertex and set its value to be an empty array

adding an edge
- This function should accept two vertices, we can call them vertex1 and vertex2
- The function should find in the adjacency list the key of vertex1 and push vertex2 to the array
- The function should find in the adjacency list the key of vertex2 and push vertex1 to the array
- Don’t worry about handling errors/ invalid vertices 

removing an edge
- This function should accept two vertices, we’ll call them vertex1 and vertex2
- The function should reassign the key of vertex1 to be an array that does not contain vertex2
- The function should reassign the key of vertex2 to be an array that does not contain vertex1
- Don’t worry about handling errors/ invalid vertices 

removing a vertex
- The function should accept a vertex to remove
- The function should loop as long as there are any other vertices in the adjacency list for that vertex
- Inside of the loop, call our removeEdge function with the vertex we are removing and any values in the adjacency list for that vertex
- delete the key in the adjacency list for that vertex

---------------------

Graph traversal

Visiting / Updating/ Checking each vertex in a graph 
Graph traversal uses
- Peer to peer networking
- Web crawlers
- Finding “closet” matches/ recommendations
- Shortest path problems (GPS navigation, Solving mazes, AI - shortest path to win the game)

Depth first graph traversal
Explore as far as possible down one branch before “backtracking”

DFS pseudocode
(recursive)
- DFS(vertex)
- if vertex is empty, return (this is base case)
- add vertex to results list
- mark vertex as visited
- for each neighbour in vertex’s neighbours: if neighbour is not visited, recursively call DFS on neighbour
more detailed
- The function should accept a starting node
- Create a list to store the end result, to be returned at the very end
- Create an object to store visited vertices
- Create a helper function which accepts a vertex
- The helper function should return early if the vertex is empty
- The helper function should place the vertex it accepts into the visited object and push that vertex into the result array
- Loop over all of the values in the adjacency list for that vertex
- If any of those values have not been visited, recursively invoke the helper function with that vertex
- Invoke the helper function with the starting vertex
- Return the result array

DFS-iterative (start)
- let S be a stack
- S.push(start)
- while S is not empty, vertex = S.pop();
- if vertex is not labelled as discovered:
- visit vertex (add to result list)
- label vertex as discovered 
- for each of vertex’s neighbors, N do S.push(N) 

more detailed
- This function should accept a starting node
- Create a stack to help use keep track of vertices (use a list / array)
- Create a list to store the end result, to be returned at the very end
- Create an object to store visited vertices
- Add the starting vertex to the stack
- While the stack has something in it
    - Pop the next vertex from the stack
    - If that vertex hasn’t been visited yet:
        - Mark it as visited
        - Add it to the result list
        - Push all of its neighbors into the stack
- Return the result array

Breadth first 
Visit neighbors at current depth first

BFS pseudocode
- This function should accept a starting vertex
- Create a queue (you can use an array) and place the starting vertex in it
- Create an array to store the nodes visited 
- Create an object to store nodes visited 
- Mark the starting vertex as visited
- Loop as long as there is anything in the queue
- Remove the first vertex from the queue and push it into the array that stores nodes visited
- Loop over each vertex in the adjacency list for the vertex you are visiting 
- If it is not inside the object that stores nodes visited, mark it as visited and enqueue that vertex
- Once you have finished looping, return the array of visited nodes

---------------------
Dijkstra’s algorithm
---------------------
What is it?
One of the most famous and widely used algorithms around. Finds the shortest path between two vertices on a graph
What’s the fastest way to get from point A to point B?

Who was he?
Edsger Dijkstra was a Dutch programmer, physicist, essayist and all around smarty-pants. 
He helped to advance the field of computer science from an “art” to an academic discipline 
Many of his discoveries and algorithms are still commonly used to this day

Why is it useful?
- GPS - finding fastest route
- Network routing - finds open shortest path for data
- Biology - used to model the spread of viruses among humans
- Airline tickets - finding cheapest route to your destination
- Many other uses

Data structure

const hashtable = {
  A: [{node: "B", weight: 10}],
  B: [{node: "A", weight: 10}]
}

The approach
1. Every time we look to visit a new node, we pick the node with the smallest known distance to visit first
2. Once we’ve moved to the node we’re going to visit, we look at each of its neighbors
3. For each neighboring node, we calculate the distance by summing the total edges that lead to the node we’re checking from the starting node
4. If the new total distance to a node is less than the previous total, we store the new shorter distance for that node

Dijkstra’s algorithm pseudocode
- This function should accept a starting and ending vertex 
- Create an object (we’ll call it distances) and set each key to be every vertex in the adjacency list with a value of infinity, except for the starting vertex which should have a value of 0
- After setting a value in the distances object, add each vertex with a priority of infinity to the priority queue, except the starting vertex, which should have a priority of 0 because that’s where we begin
- Create another object called previous and set each key to be every vertex in the adjacency list with a value of null
- Start looping as long as there is anything in the priority queue
    - dequeue a vertex from the priority queue
    - If that vertex is the same as the ending vertex - we are done
    - Otherwise loop through each value in the adjacency list at that vertex
        - Calculate the distance to that vertex from the starting vertex
        - If the distance is less than what is currently stored in our distances object
            - Update the distances object with new lower distance
            - Update the previous object to contain that vertex
            - Enqueue the vertex with the total distance from the start node
