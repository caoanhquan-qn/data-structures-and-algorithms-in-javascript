Data Structures and Algorithms in Javascript
---------------------
Big O Notation

In computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows
time complexity

input size = N

1. O(1)
2. O(log n) 
4. O(n)
6. O(n log n)
8. O(n^2) (quadratic)

big O time complexity 

constant > log n > √n > n > n log n > n^2 > 2^n > n! 

option + v : √

best sorting algorithm = O(N log(N)) (such as merge sort, quick sort)
search a table of N rows = O(N)
print array of N = O(N)

b-tree : the core of most databases implementations 
b-tree : a self-balancing tree data structures that keeps data sorted and allows searches, sequential access, insertions and deletions in logarithmic time. 
It is commonly used in databases and filesystems

b-tree of order m = 3, meaning a node has at most 3 children 

space complexity 
---------------------

what happens to the space that an algorithm takes up as the size of the input increases
we can also use big O notation to analyze space complexity: how much additional memory do we need to allocate in order to run the code in our algorithm?

sometimes you’ll hear the term auxiliary space complexity to refer to space required by the algorithm, not including space taken up by the inputs

unless otherwise noted, when we talk about space complexity, technically we’ll be talking about auxiliary space complexity 

space complexity in JS (rules of thumb)
- most primitives (booleans, numbers, undefined, null) are constant space
- Strings require O(n) space (where n is the string length)
- Reference types are generally O(n), where n is the length (for arrays) or the number of keys (for objects)

logarithm
this isn’t a math course, so here’s a rule of thumb
the logarithm of a number roughly measures the number of times you can divide that number by 2 before you get a value that’s less than or equal to one

=> logarithm complexity

who cares?
- certain searching algorithms have logarithmic time complexity
- efficient sorting algorithms involve logarithms 
- recursion sometimes involves logarithmic space complexity 

Recap
- To analyze the performance of an algorithm, we use Big O Notation
- Big O Notation can give us a high level understanding of the time or space complexity of an algorithm
- Big O Notation doesn’t care about precision, only about general trends  (linear? quadratic? constant?)
- The time or space complexity (as measured by Big O) depends only on the algorithm, not the hardware used to run the algorithm
- Big O Notation is everywhere, so get lots of practice

---------------------

when to use objects
- when you don’t need order
- when you need fast access/ insertion and removal

Big O of Objects

insertion - O(1)
removal - O(1)
searching - O(n)
access - O(1)
update - O(1)

when you don’t need any ordering, objects are an excellent choice!

Big O of Object methods

Object.keys - O(n)
Object.values - O(n)
Object.entries - O(n)
Object.hasOwnProperty - O(1)

---------------------

const jonas = {
  firstName: "Nick",
  age: 32,
  isSingle: true,
  score: [49, 64, 75, 85],
};

console.log(Object.keys(jonas));

// ["firstName", "age", "isSingle", "score"]

console.log(Object.values(jonas));

// ["Nick", 32, true, Array(4)]

console.log(Object.entries(jonas));

/*
0: (2) ["firstName", "Nick"]
1: (2) ["age", 32]
2: (2) ["isSingle", true]
3: (2) ["score", Array(4)]
*/

console.log(jonas.hasOwnProperty("score")); // true

---------------------

when to use arrays
- when you need order 
- when you need fast access/ insertion and removal

Big O of arrays

insertion - it depends
depends on where we’re inserting/removing 

if we add to the end of the array => O(1)
the problem comes when we try to insert at the beginning of an array. we have to re-index every single element in the array. 
If we’re talking about thousands and thousands of elements, re-indexing every single one is not a trivial task => O(n)
inserting in the beginning is problematic. the same goes for removing from the beginning ⬇︎

removal - it depends
remove from the beginning => O(n)
remove the last element => O(1)


=> adding and removing from the beginning of an array is best to avoid if you can. So push and pop are always faster than unshift and shift
push - unshift
pop - shift

unless it’s an empty array in which case adding to the beginning or end is the same thing

searching - O(n)

access - O(1)
it doesn’t matter how long the array is => jump immediately to the data 

Big O of array operations
- push - O(1)
- pop - O(1)
- shift - O(n)
- unshift - O(n)
- concat - O(n)
- slice - O(n)
- splice - O(n)
- sort - O(n * log n)
- forEach/ map/ filter/ reduce/ ... - O(n)

---------------------
Problem solving pattern
- Frequency counter
- Multiple pointers
- Sliding window
- Divide and conquer
- Dynamic programming
- Greedy algorithms
- Backtracking
- Many more! 

Frequency counter
This pattern uses objects or sets to collect values/ frequencies of values. This can often avoid the need for nested loops or O(n^2) operations with arrays/ strings

Multiple pointers (it’s not really an official name)

Creating pointers or values that correspond to an index or position and move towards the beginning, end or middle based on a certain condition. 
Very efficient for solving problems with minimal space complexity as well

Sliding window
This pattern involves creating a window which can either be an array or number from one position to another. 
Depending on a certain condition, the window either increases or closes (and a new window is created). Very useful for keeping track of a subset of data in an array/ string

Divide and Conquer pattern
This pattern involves dividing a data set into smaller chunks and then repeating a process with a subset of data. This pattern can tremendously decrease time complexity

---------------------
Recursion
What is recursion?
A process (a function in our case) that calls itself

How recursive functions work?
Invoke the same function with a different input until you reach your base case
base case: the condition when the recursion ends

Two essential parts of a recursive function
- base case
- different input

Common recursion pitfalls
- No base case
- Forgetting to return or returning the wrong thing
- stack overflow

stack overflow : maximum call stack size exceeded, too many frames on that stack, too many functions trying to be called. This means that your recursion is not stopped

Pure recursion tips
- For arrays, use methods like slice, the spread operator and concat that make copies of arrays so you do not mutate them
- Remember that strings are immutable so you will need to use methods like slice, substr or substring to make copies of strings
- To make copies of objects use Object.assign or the spread operator

---------------------

Searching Algorithms
- Linear search : it’s the best we can do with an unsorted piece of data
- Binary Search : only works on sorted arrays. The idea is divide and conquer pattern. Big O : O(log n)
- Naive string searching algorithm
- KMP string searching algorithm

There are many different search methods on arrays in Javascript
- indexOf
- includes
- find
- findIndex

How do these functions work?
All of these methods are doing the same thing behind the scenes.
They’re checking every element one at a time to check if whatever we pass in. 
They are going through and checking each item to see. It’s called a linear search.  
---------------------

Sorting Algorithms
https://visualgo.net/en/sorting
https://www.toptal.com/developers/sorting-algorithms

[
1. Bubble sort
2. Selection sort
3. Insertion sort
4. Shell sort ] => elementary sorting algorithms
[
5. Merge sort
6. Quick sort
7. Radix sort
8. Heap sort ] => intermediate sorting algorithms

Bubble sort : a sorting algorithm where the largest values bubble up to the top
Bubble sort pseudocode
- Start looping from the end of the array towards the beginning with a variable called i 
- Start an inner loop with a variable called j from the beginning until i - 1
-  If arr[j] is greater than arr[j+1], swap those two values
- Return the sorted array

What is the time complexity of bubble sort?
best case: O(n) where the data is nearly sorted 
worst case: O(n^2). In general, n^2 because we have a nested loop 

Selection sort : similar to bubble sort, but instead of first placing large values into sorted position at the end of the array, it places small values into sorted position at the beginning

Selection sort pseudocode
- Store the first element as the smallest value you’ve seen so far
- Compare this item to the next item in the array until you find a smaller number
- If a smaller number is found, designate that smaller number to be the new “minimum” and continue until the end of the array
- If the “minimum” is not the value (index) you initially began with, swap the two values
- Repeat this with the next element until the array is sorted

What is the time complexity of selection sort?
O(n^2)
selection sort potentially is better than something like bubble sort because we minimize the number of swaps that we’re making
bubble sort : we’re basically swapping over and over and over to get the largest item to the end, we keep swapping until it goes to the end and the next time through we swap if we need to keep swapping 

selection sort : we iterate we compare a lot but we only make one swap at the end of each loop   

Insertion sort : builds up the sort by gradually creating a larger left half which is always sorted 
Insertion sort pseudocode
- Start by picking the second element in the array
- Now compare the second element with the one before it and swap if necessary
- Continue to the next element and if it is in the incorrect order, iterate through the sorted portion (i.e. the left side) to place the element in the correct place
- Repeat until the array is sorted

What is the time complexity of insertion sort?
O(n^2)
It works very well if you need your data to be continuously sorted. You have data coming in and you need to re-sort things over and over and maintain a running sort, keep things up-to-date.

Insertion sort is an online algorithm
Selection sort is an offline algorithm

Merge sort
divide and conquer pattern 
https://www.bigocheatsheet.com/

mergeSort pseudocode
- Break up the array into halves until you have arrays that are empty or have one element (use slice)
- Once you have smaller sorted arrays, merge those arrays with other sorted arrays until you are back at the full length of the array
- Once the array has been merged back together, return the merged and sorted array

What is the time complexity of merge sort?
O(n log n)
space complexity : O(n) 

Quick sort
Quick sort pseudocode
- Call the pivot helper on the array
- When the helper returns to you the updated pivot index, recursively call the pivot helper on the subarray to the left of that index, and the subarray to the right of that index
- Your base case occurs when you consider a subarray with less than 2 elements

What is the time complexity of quick sort?
O(n log n)
worst case : O(n^2) where our data is already sorted 

Radix sort 
Radix sort pseudocode
- Define a function that accepts list of numbers
- Figure out how many digits the largest number has 
- Loop from k = 0 up to this largest number of digits
- For each iteration of the loop:
    - Create buckets for each digit (0 to 9)
    - Place each number in the corresponding bucket based on its k-th digit
- Replace our existing array with values in our buckets, starting with 0 and going up to 9
- Return list at the end

What is the time complexity of radix sort?
O(nk)
space complexity : O(n + k)
n = length of array
k = number of digits 

---------------------

Singly Linked List
---------------------

It’s a data structure that stores whatever data you want such as strings, numbers. It’s ordered. 
It’s a list of data just like an array. But there’s a really big distinction.
A data structure that contains a head, tail, and length property. 
Linked lists consist of nodes and each node has a value and a pointer to another node or null.
A node stores a piece of data like a string or a number but it also references the next node. 
Singly linked list comes from the fact that each node is only connected one directionally to the next node.
One of the things that linked lists are good at is insertion and deletion. 
One of the main reasons you would want to use linked lists is that you really care about insertion and deletion, 
especially if you’re working with really long piece of data and you don’t need random access.

Comparison with arrays

Lists	
- Do not have indexes	
- Connected via nodes with a next pointer	
- Random access is not allowed	
Arrays
- Indexed in order 
- Insertion and deletion can be expensive
- Can quickly be accessed at a specific index

Singly linked list methods
- push(val) : adding a new node to the end of the linked list
- pop() : removing a node from the end of the linked list
- shift() : removing a node from the beginning of the linked list 

pushing pseudocode
- This function should accept a value
- Create a new node using the value passed to the function
- If there is no head property on the list, set the head and tail to be the newly created node
- Otherwise set the next property on the tail to be the new node and set the tail property on the list to be the newly created node
- Increment the length by 1
- Return the linked list

popping pseudocode
- If there are no nodes in the list, return undefined
- Check if this.length === 1 (edge case)
- Loop through the list until you reach the tail
- Set the next property of the 2nd to last node to be null
- Set the tail to be the 2nd to last node
- Decrement the length of the list by 1
- Return the value of the node removed

shifting pseudocode
- If there are no nodes, return undefined
- Store the current head property in a variable
- Set the head property to be the current head’s next property
- Decrement the length by 1
- Return the value of the node removed

Big O of Singly Linked List
Insertion - O(1)
Removal - It depends ... O(1) or O(n)
Searching - O(n)
Access - O(n)
---------------------

Big O of Array
Insertion -  It depends ... O(1) or O(n)
Removal -  It depends ... O(1) or O(n)
Searching - O(n)
Access - O(1)

Recap
- Singly linked lists are an excellent alternative to arrays when insertion and deletion at the beginning are frequently required
- Arrays contain a built-in index whereas linked lists don’t
- The idea of a list data structure that consists of nodes is the foundation for other data structures like stacks and queues
